{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b919a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 3 documents (files).\n",
      "‚úÖ Split into 27 smaller chunks.\n",
      "\n",
      "--- Sample Chunk ---\n",
      "# About Ankit Sharma\n",
      "\n",
      "Born: January 8, 2005 (Age 21)  \n",
      "Gender: Male  \n",
      "Religion: Hindu  \n",
      "Category: General  \n",
      "From: VPO Karloti, Tehsil Ghumarwin, District Bilaspur, Himachal Pradesh 174029\n",
      "\n",
      "Passionate B.Tech Computer Science student from VIT Bhopal University with strong academic record and professional certifications. Currently building expertise in AI/ML through research internships and self-driven projects. Cricket enthusiast, sports lover, and aspiring family provider.\n",
      "\n",
      "---\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load the data from your 'data' folder\n",
    "# glob=\"*.md\" ensures we only read markdown files\n",
    "loader = DirectoryLoader(\n",
    "    './data', \n",
    "    glob=\"*.md\", \n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(documents)} documents (files).\")\n",
    "\n",
    "# 2. Split the data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Characters per chunk\n",
    "    chunk_overlap=50 # Overlap to keep context between chunks\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"‚úÖ Split into {len(chunks)} smaller chunks.\")\n",
    "\n",
    "# 3. Inspect the first chunk to verify\n",
    "print(\"\\n--- Sample Chunk ---\")\n",
    "print(chunks[0].page_content)\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efeb6af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading embedding model...\n",
      "‚è≥ Creating vector index from chunks...\n",
      "‚úÖ Success! Vector Store saved to folder: 'faiss_index'\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. Define the Embedding Model\n",
    "# We use \"all-MiniLM-L6-v2\". It is the industry standard for fast, free, local embeddings.\n",
    "print(\"‚è≥ Loading embedding model...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Create the Vector Database\n",
    "# This converts your 27 chunks into vectors.\n",
    "print(\"‚è≥ Creating vector index from chunks...\")\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# 3. Save it to disk\n",
    "# This allows us to load it later without re-running this code.\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Success! Vector Store saved to folder: 'faiss_index'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d9351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Vector Store...\n",
      "\n",
      "üîç Query: Tell me about the Brain Tumor Detection project\n",
      "----------------------------------------\n",
      "üìÑ Result 1 (Source: data\\project.md):\n",
      "# Computer Vision & Medical AI\n",
      "\n",
      "## Brain Tumor Detection System (Featured Project)\n",
      "\n",
      "Multi-class MRI tumor classification with 94%+ accuracy using CNNs.\n",
      "\n",
      "**Problem:** Classify brain MRI images into glioma, meningioma, pituitary, and no tumor.\n",
      "\n",
      "**Dataset:** 3,000+ T1-weighted MRI scans with augmentati...\n",
      "----------------------------------------\n",
      "üìÑ Result 2 (Source: data\\project.md):\n",
      "**Architecture:**\n",
      "\n",
      "- Custom CNN: Conv2D √¢‚Ä†‚Äô MaxPool √¢‚Ä†‚Äô Dropout √¢‚Ä†‚Äô Dense layers\n",
      "- Transfer Learning: VGG16 / ResNet50 fine-tuned\n",
      "- Preprocessing: Resize 150x150, normalization, CLAHE enhancement\n",
      "\n",
      "**Performance Metrics:**\n",
      "\n",
      "- Test Accuracy: 94.23% (1324/1405 images)\n",
      "- Validation Accuracy: 91√¢‚Ç¨‚Äú94%\n",
      "- ...\n",
      "----------------------------------------\n",
      "üìÑ Result 3 (Source: data\\project.md):\n",
      "**Tech Stack:** PyTorch, OpenCV, CASIA-B preprocessing scripts.\n",
      "\n",
      "Status: Active development....\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# 1. Load the Embedding Model (Same one used to create the index)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Load your saved Vector Store\n",
    "# allow_dangerous_deserialization=True is required for local files (safe here since you created them)\n",
    "print(\"‚è≥ Loading Vector Store...\")\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 3. Run a Test Search\n",
    "query = \"Tell me about the Brain Tumor Detection project\"\n",
    "# k=3 means \"Give me the top 3 most relevant chunks\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"\\nüîç Query: {query}\")\n",
    "print(\"-\" * 40)\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"üìÑ Result {i+1} (Source: {doc.metadata['source']}):\")\n",
    "    print(doc.page_content[:300] + \"...\")  # Show first 300 chars\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02eb2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Reloading Vector Store...\n",
      "‚úÖ SUCCESS! Chain built with InferenceClient.\n",
      "ü§ñ Testing...\n",
      "\n",
      "========================================\n",
      "I'm sorry but the provided context doesn't include any information about Ankit's sister's name. I'd say, \"I don't have that info. And let's keep the answer in a chill moood by suggesting that Ankit seems to be quite focused on his academic, professional, and personal pursuits, but we're not privy to his sister's name at this time. Maybe you can ask him directly or do some sleuthing on social media to uncover that secret detail, but for now, let's just focus on celebrating his impressive accomplishments and talents in sports, music, and cooking! üç≥üéßüèÉ‚Äç‚ôÇÔ∏èüèãÔ∏è‚Äç‚ôÇÔ∏èüé§üèÄüç≥ #ankitsharma1831ao #chillvibes #passionatelearner #selfdriven #himachalipride #techwhizkid #chefankitsharma\" üòéü§©ü§©ü§©ü§© #keepcalmandcookon #fitnessfun #githubguru\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from huggingface_hub import InferenceClient # <--- The Official Client\n",
    "\n",
    "# 1. Setup API Key\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_TOKEN\"] = getpass(\"üîë Enter your Hugging Face Token: \")\n",
    "\n",
    "# 2. Reload Vector Store\n",
    "print(\"‚è≥ Reloading Vector Store...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. Define the Chat Function (The \"Fix\")\n",
    "# We use the official client to talk to Zephyr-7B\n",
    "def query_zephyr_chat(prompt_text):\n",
    "    client = InferenceClient(api_key=os.environ[\"HF_TOKEN\"])\n",
    "    \n",
    "    # We construct a simple \"User\" message\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_text}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # We use 'chat_completion' which is the modern standard for these models\n",
    "        response = client.chat_completion(\n",
    "            model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            messages=messages,\n",
    "            max_tokens=500,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        # Extract the answer text\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# 4. Define Prompt\n",
    "template = \"\"\"You are Ankit's AI Assistant. Answer strictly based on the context.\n",
    "If you don't know, say \"I don't have that info. and keep the answer in fun way , in chill moood\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 5. Build the Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | (lambda x: x.to_string())  # Convert Prompt to String\n",
    "    | query_zephyr_chat          # Send to Zephyr Chat\n",
    ")\n",
    "\n",
    "print(\"‚úÖ SUCCESS! Chain built with InferenceClient.\")\n",
    "\n",
    "# 6. Test Immediately\n",
    "print(\"ü§ñ Testing...\")\n",
    "response = rag_chain.invoke(\"what is ankit sister name ?\")\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(response)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a44b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3328c119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a256ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c9eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006144d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1566c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
